# ğŸ¤– LLM Interface with Ollama

A beautiful web interface to interact with Large Language Models through Ollama.

![Python](https://img.shields.io/badge/Python-3.8+-blue?logo=python&logoColor=white)
![Flask](https://img.shields.io/badge/Flask-3.0.2-green?logo=flask&logoColor=white)
![SQLite](https://img.shields.io/badge/SQLite-Database-orange?logo=sqlite&logoColor=white)

## âœ¨ Features

- ğŸŒ **Intuitive Web Interface** - Clean and modern UI for seamless interaction
- ğŸ“ **Markdown Support** - Rich text formatting in responses
- ğŸ¨ **Syntax Highlighting** - Beautiful code block rendering
- ğŸ”§ **Extensible Architecture** - Easy to add new models and features
- ğŸ’¾ **Conversation History** - (Coming soon)
- ğŸ“¤ **Export Conversations** - (Coming soon)
- âš¡ **Real-time Response Display** - (Coming soon)

## ğŸ—ï¸ Architecture Overview

This project follows a clean Flask application structure with separation of concerns:

```
LLM-interface/
â”œâ”€â”€ ğŸ“ app/                    # Main application package
â”‚   â”œâ”€â”€ ğŸ“ main/              # Main blueprint (web interface)
â”‚   â”‚   â”œâ”€â”€ __init__.py       # Blueprint initialization
â”‚   â”‚   â””â”€â”€ routes.py         # Web routes and view functions
â”‚   â”œâ”€â”€ ğŸ“ api/               # API blueprint (REST endpoints)
â”‚   â”‚   â”œâ”€â”€ __init__.py       # API blueprint initialization
â”‚   â”‚   â””â”€â”€ routes.py         # API endpoints
â”‚   â”œâ”€â”€ ğŸ“ templates/         # Jinja2 HTML templates
â”‚   â”‚   â””â”€â”€ index.html        # Main chat interface
â”‚   â”œâ”€â”€ ğŸ“ static/           # Static assets
â”‚   â”‚   â”œâ”€â”€ css/             # Stylesheets
â”‚   â”‚   â””â”€â”€ js/              # JavaScript files
â”‚   â”œâ”€â”€ ğŸ“ utils/            # Utility functions
â”‚   â”œâ”€â”€ __init__.py          # App factory pattern
â”‚   â””â”€â”€ models.py            # SQLAlchemy database models
â”œâ”€â”€ ğŸ—„ï¸ app.db                # SQLite database
â”œâ”€â”€ âš™ï¸ config.py             # Configuration settings
â”œâ”€â”€ ğŸš€ run.py                # Application entry point
â””â”€â”€ ğŸ“‹ requirements.txt      # Python dependencies
```

### ğŸ”§ Core Components

#### ğŸ­ **App Factory Pattern** (`app/__init__.py`)

- Uses Flask's application factory for better modularity
- Initializes SQLAlchemy database
- Registers blueprints for main and API routes

#### ğŸ—ƒï¸ **Database Models** (`app/models.py`)

- **Conversation**: Stores chat sessions with timestamps
- **Message**: Individual messages with user/AI distinction and model tracking
- Relationship: One conversation has many messages

#### ğŸŒ **Web Interface** (`app/main/routes.py`)

- Serves the main chat interface
- Handles conversation display and management

#### ğŸ”Œ **API Layer** (`app/api/routes.py`)

- RESTful endpoints for frontend communication
- Handles Ollama integration and message processing

#### âš™ï¸ **Configuration** (`config.py`)

- Environment-based configuration
- Ollama URL and model settings
- Database connection strings

## ğŸ”„ How It Works

1. **ğŸŒŸ User Interaction**: User types a message in the web interface
2. **ğŸ“¡ API Call**: Frontend sends POST request to `/api/chat` endpoint
3. **ğŸ¤– Ollama Integration**: Backend forwards request to Ollama server
4. **ğŸ’¾ Data Persistence**: Conversation and messages are saved to SQLite database
5. **ğŸ“¤ Response Streaming**: AI response is streamed back to the frontend
6. **ğŸ¨ Rendering**: Markdown content is rendered with syntax highlighting

## ğŸš€ Prerequisites

- ğŸ **Python 3.8+**
- ğŸ¦™ **Ollama** installed and running on port 11434
- ğŸ¤– **llama3** model loaded in Ollama

## ğŸ“¦ Installation

1. **Clone the repository:**

```bash
git clone https://github.com/julienBelinga/LLM-interface.git
cd LLM-interface
```

2. **Create virtual environment:**

```bash
python -m venv venv

# On Linux/Mac
source venv/bin/activate

# On Windows
.\venv\Scripts\activate
```

3. **Install dependencies:**

```bash
pip install -r requirements.txt
```

4. **Environment setup:**

```bash
cp .env.example .env
# Edit .env file with your configuration
```

## ğŸ¯ Quick Start

```bash
python run.py
```

The application will be available at **http://localhost:5000** ğŸŒ

## ğŸ”§ Configuration

The application can be configured through environment variables in `.env`:

```env
SECRET_KEY=your-secret-key-here
DATABASE_URL=sqlite:///app.db
OLLAMA_URL=http://localhost:11434
DEFAULT_MODEL=llama3
```

## ğŸš€ Future Enhancements

- ğŸ“¤ **Export Conversations** - Save chats as PDF/Markdown
- ğŸ” **Search History** - Find specific conversations quickly
- ğŸ¨ **Theme Customization** - Dark/Light mode support
- ğŸ“Š **Analytics Dashboard** - Usage statistics and insights
- ğŸ” **User Authentication** - Multi-user support
- ğŸŒ **Multi-language Support** - Internationalization

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is open source and available under the MIT License.

---

Made with â¤ï¸ and ğŸ¤–
